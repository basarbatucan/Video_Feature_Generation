{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e66f2d-3c87-415e-819e-1bf29bce9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Warning this notebook takes 21 hours 40 minutes to complete for Shangai dataset w AMD3900x, Nvidia GeForce RTX 2070, 32Gb ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf69058-6278-4498-bf56-7333509cce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "data_path = './data/ShangaiTech/'\n",
    "train_path = data_path + 'training/videos/'\n",
    "test_path = data_path + 'testing/'\n",
    "test_path_frame = test_path + 'frames/'\n",
    "test_path_pixel_mask = test_path + 'test_pixel_mask/'\n",
    "\n",
    "output = './output/ShangaiTech/'\n",
    "output_test_path = output + 'Test/'\n",
    "output_train_path = output + 'Train/'\n",
    "\n",
    "DAE_in_size = 64\n",
    "min_box_size_ratio = 0.002\n",
    "valid_perc = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53f1161-6e4d-4d7e-8b48-7e829e749c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\basarbatu/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2021-11-3 torch 1.10.0+cu113 CUDA:0 (NVIDIA GeForce RTX 2070 SUPER, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46533693 parameters, 0 gradients, 109.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5l')\n",
    "\n",
    "# define confidence of the model\n",
    "model.conf = 0.3  # confidence threshold (0-1)\n",
    "model.iou = 0.5   # NMS IoU threshold (0-1)\n",
    "\n",
    "# define classes here\n",
    "model.classes = [0, 1, 2, 3, 5, 6, 7, 8, 14, 15, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30dfcd8-c8da-44d0-9323-874347c99ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821.76\n"
     ]
    }
   ],
   "source": [
    "# video files\n",
    "train_video_dirs = [train_path+x for x in os.listdir(train_path) if os.path.isfile(os.path.join(train_path, x))]\n",
    "test_frame_dirs = [test_path_frame+x for x in os.listdir(test_path_frame) if os.path.isdir(os.path.join(test_path_frame, x))]\n",
    "images = [x for x in os.listdir(test_frame_dirs[0]) if x[0]!='.']\n",
    "input_img_path = test_frame_dirs[0] + '/' + images[0]\n",
    "img = cv2.imread(input_img_path)\n",
    "min_box_size = img.shape[0]*img.shape[1]*min_box_size_ratio\n",
    "print(min_box_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb1e6b1-1be9-4d07-b456-7eb124499e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 330/330 [16:07:32<00:00, 175.92s/it]\n"
     ]
    }
   ],
   "source": [
    "d = {'xmin': [], 'ymin': [], 'xmax': [], 'ymax': [], 'area': [], 'confidence':[], 'class':[], 'name':[], 'ori_im_path':[], 'scaled_im_path':[], 'label':[], 'frame_n':[], 'train_valid_test':[]}\n",
    "data_meta = pd.DataFrame(data=d)\n",
    "frame_index = 0\n",
    "yolo_obj_index = 0\n",
    "\n",
    "for train_video_dir in tqdm(train_video_dirs):\n",
    "    # Read video\n",
    "    vidcap = cv2.VideoCapture(train_video_dir)\n",
    "    # Read the first image\n",
    "    success,img = vidcap.read()\n",
    "    while success:\n",
    "        # make prediction\n",
    "        results = model(img)\n",
    "        out = results.pandas().xyxy[0]\n",
    "        # calculate area\n",
    "        out['area'] = (out['xmax']-out['xmin'])*(out['ymax']-out['ymin'])\n",
    "        # update common info\n",
    "        out['train_valid_test'] = 'train'\n",
    "        out['frame_n'] = frame_index\n",
    "        out['ori_im_path'] = train_video_dir\n",
    "        # append meta data\n",
    "        data_meta = data_meta.append(out, ignore_index=True)\n",
    "        # go through the all detections\n",
    "        for j in range(0, len(out)):\n",
    "            # get the cropped image\n",
    "            xmin = int(out.loc[j, 'xmin'])\n",
    "            xmax = int(out.loc[j, 'xmax'])\n",
    "            ymin = int(out.loc[j, 'ymin'])\n",
    "            ymax = int(out.loc[j, 'ymax'])\n",
    "            # extract image\n",
    "            yolo_object_img = img[ymin:ymax, xmin:xmax, :]\n",
    "            # rescale the cropped image\n",
    "            resized_yolo_object_img = cv2.resize(yolo_object_img, (DAE_in_size, DAE_in_size), interpolation = cv2.INTER_AREA)\n",
    "            # save the scaled images\n",
    "            saved_path = output_train_path + '{:06d}.png'.format(yolo_obj_index)\n",
    "            cv2.imwrite(saved_path, resized_yolo_object_img)\n",
    "            data_meta.loc[yolo_obj_index, 'scaled_im_path'] = saved_path\n",
    "            # update meta data for label\n",
    "            data_meta.loc[yolo_obj_index, 'label'] = 0\n",
    "            yolo_obj_index = yolo_obj_index+1\n",
    "        frame_index = frame_index+1\n",
    "        # Read the next image\n",
    "        success,img = vidcap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a7889c-bdc2-46b9-9ef7-d5d86c9c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign last percentage of the training set as validation\n",
    "\n",
    "n_valid = round(len(data_meta)*valid_perc)\n",
    "valid_indices = data_meta.sample(n_valid).index\n",
    "data_meta.loc[valid_indices, 'train_valid_test'] = 'valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc1457b-f903-445e-83ac-aa4556b4326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 107/107 [5:32:22<00:00, 186.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object detection for test images completed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for test_frame_dir in tqdm(test_frame_dirs):\n",
    "    images = [x for x in os.listdir(test_frame_dir) if x[0]!='.']\n",
    "    pixel_masks = np.load(test_path_pixel_mask + test_frame_dir.split('/')[-1] + '.npy')\n",
    "    for i in range(0, len(images)):\n",
    "        # read image\n",
    "        input_img_path = test_frame_dir + '/' + images[i]\n",
    "        img = cv2.imread(input_img_path)\n",
    "        # read annot\n",
    "        label = pixel_masks[i,:,:]\n",
    "        # make prediction\n",
    "        results = model(img)\n",
    "        out = results.pandas().xyxy[0]\n",
    "        # calculate area\n",
    "        out['area'] = (out['xmax']-out['xmin'])*(out['ymax']-out['ymin'])\n",
    "        # update common info\n",
    "        out['train_valid_test'] = 'test'\n",
    "        out['frame_n'] = frame_index\n",
    "        out['ori_im_path'] = input_img_path\n",
    "        # append meta data\n",
    "        data_meta = data_meta.append(out, ignore_index=True)\n",
    "        # go through the all detections\n",
    "        for j in range(0, len(out)):\n",
    "            # get the cropped image\n",
    "            xmin = int(out.loc[j, 'xmin'])\n",
    "            xmax = int(out.loc[j, 'xmax'])\n",
    "            ymin = int(out.loc[j, 'ymin'])\n",
    "            ymax = int(out.loc[j, 'ymax'])\n",
    "            # extract image\n",
    "            yolo_object_img = img[ymin:ymax, xmin:xmax, :]\n",
    "            yolo_object_label = label[ymin:ymax, xmin:xmax]\n",
    "            # rescale the cropped image\n",
    "            resized_yolo_object_img = cv2.resize(yolo_object_img, (DAE_in_size, DAE_in_size), interpolation = cv2.INTER_AREA)\n",
    "            # save the scaled images\n",
    "            saved_path = output_test_path + '{:06d}.png'.format(yolo_obj_index)\n",
    "            cv2.imwrite(saved_path, resized_yolo_object_img)\n",
    "            data_meta.loc[yolo_obj_index, 'scaled_im_path'] = saved_path\n",
    "            # update meta data for label\n",
    "            is_abnormal = np.sum(yolo_object_label)/(yolo_object_label.shape[0]*yolo_object_label.shape[1])\n",
    "            data_meta.loc[yolo_obj_index, 'label'] = is_abnormal\n",
    "            yolo_obj_index = yolo_obj_index+1\n",
    "        frame_index = frame_index+1\n",
    "print('Object detection for test images completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff70feec-e979-4598-b9a5-61139a3d66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle train valid and test sets within their groups\n",
    "train_data = data_meta[data_meta['train_valid_test']=='train'].sample(frac=1)\n",
    "valid_data = data_meta[data_meta['train_valid_test']=='valid'].sample(frac=1)\n",
    "test_data = data_meta[data_meta['train_valid_test']=='test'].sample(frac=1)\n",
    "data_meta = pd.concat([train_data, valid_data, test_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c127d1-c6f6-4189-a6d9-a235a776a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row number dropped from 1245547 to 1009732\n"
     ]
    }
   ],
   "source": [
    "# Exclude the small area yolo objects\n",
    "n1 = data_meta.shape[0]\n",
    "data_meta = data_meta[data_meta['area']>min_box_size]\n",
    "n2 = data_meta.shape[0]\n",
    "print('Total row number dropped from {} to {}'.format(n1,n2))\n",
    "# Save the meta file\n",
    "data_meta.to_csv(output + 'meta_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
